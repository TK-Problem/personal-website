[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "One-Sample Binomial Test (Part 1)\n\n\n\n\n\n\n\nR\n\n\nPython\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nTomas Kristijonas Uždavinys\n\n\n\n\n\n\n  \n\n\n\n\nExports in Lithuania\n\n\n\n\n\n\n\nR\n\n\nMacro Economy\n\n\nCoursera\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2023\n\n\nTomas Kristijonas Uždavinys\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tomas K. Uždavinys",
    "section": "",
    "text": "I am a physics scientist who became a Python developer and risk analyst. My principal expertise is in writing predictive models, web scraping and automation scripts. I also have a passion to explain complicated topics in an easy-to-understand manner."
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Tomas K. Uždavinys",
    "section": "Experience",
    "text": "Experience\nQuantitative Risk Analyst | SEB | 2022 JAN - Present\nFreelancer | MB “Kristaulitai” | 2019 JAN - Present"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Tomas K. Uždavinys",
    "section": "Education",
    "text": "Education\nKTH - Royal institute of technology | PhD, Photonics | 2014 SEP - 2018 AUG\nVilnius University | M.S., Optoelectronic materials and technology | 2012 SEP - 2014 AUG"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "PhD thesis [download PDF]\nPhD defense [recording on YouTube]"
  },
  {
    "objectID": "portfolio.html#papers",
    "href": "portfolio.html#papers",
    "title": "Portfolio",
    "section": "Papers",
    "text": "Papers\n\nS. Marcinkevičius, T. K. Uždavinys, H. M. Foronda, D. A. Cohen, C. Weisbuch, and J. S. Speck, “Intervalley energy of GaN conduction band measured by femtosecond pump-probe spectroscopy”, Phys. Rev. B 94, 235205 (2016)\nT. K. Uždavinys, S. Marcinkevičius, J. H. Leach, K. R. Evans, and D. C. Look, “Photoexcited carrier trapping and recombination at Fe centers in GaN”, J. Appl. Phys. 119, 215706 (2016)\nR. Butté, L. Lahourcade, T. K. Uždavinys, G. Callsen, M. Mensi, M. Glauser, G. Rossbach, D. Martin, J-F. Carlin, S. Marcinkevičius and N. Grandjean, “Optical absorption edge broadening in thick In- GaN layers: Random alloy atomic disorder and growth mode induced fluctuations”, Appl. Phys. Lett., 112, 032106 (2018).\nT. K. Uždavinys, S. Marcinkevičius, M. Mensi, L. Lahourcade, J-F. Carlin, D. Martin, R. Butté and N. Grandjean, “Impact of surface morphology on properties of light emission in InGaN epilayers”, Appl. Phys. Express 11, 051004 (2018).\nR. Ivanov, S. Marcinkevičius, T. K. Uždavinys, L. Y. Kuritzky, S. Nakamura, and J. S. Speck, “Scanning near-field microscopy of carrier lifetimes in m-plane InGaN quantum wells”, Appl. Phys. Lett. 110, 031109 (2017).\nT. K. Uždavinys, D. L. Becerra, R. Ivanov, S. P. DenBaars, S. Nakamura, J. S. Speck, and S. Marcinkevičius, “Influence of well width fluctuations on recombination properties in semipolar InGaN quantum wells studied by time- and spatially-resolved near-field photoluminecence”, Opt. Mat. Express, 7(9), 3116-3123 (2017)."
  },
  {
    "objectID": "posts/binom-test-1/index.html",
    "href": "posts/binom-test-1/index.html",
    "title": "One-Sample Binomial Test (Part 1)",
    "section": "",
    "text": "The binomial test is a statistical test used to determine whether the proportion of cases in one of only two categories is equivalent to a pre-specified proportion. Categories could include the default rate of clients within the next 12 months, patients with high or low risk of heart disease, potential customers who are likely or not likely to make a purchase, or the rate of manufacturing defects. This widely used test finds applications in diverse fields, including credit risk, medicine, and manufacturing. It is also known to as the one-sample proportion test or test of one proportion.\nAs with all statistical tests, the binomial test has assumptions and conditions that must be met before applying it to real-life data:\n\nThe “success-failure” condition requires observing a minimum of n successes and n failures in the sample;\nobservations are independent, i.e. the occurrence of one event does not affect the probability of occurrence of the other.\n\nThe aim of this blog post is to showcase the ramifications of failing to meet “success-failure” condition critereon. Practical examples are coded in both R and Python languages.\n\nRPython\n\n\n\n\nCode\n# load libraries for blog post\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(data.table)\nlibrary(kableExtra)\nlibrary(RColorBrewer) # for generating color palettes\n\n\n\n\n\n\nCode\n# load packages for blog post\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom scipy.stats import binom, norm\n\n# set style\nplt.style.use(\"ggplot\")"
  },
  {
    "objectID": "posts/binom-test-1/index.html#theory",
    "href": "posts/binom-test-1/index.html#theory",
    "title": "One-Sample Binomial Test (Part 1)",
    "section": "Theory",
    "text": "Theory\nSuppose that we have a sample where outcomes are binary - e.g. only “success” and “failure”. For the given sample, we would like to estimate the true proportion and also set up a statistical test to verify whether if the proportion is equal to some value, e.g. expected.\nFirst, we calculate a point estimate:\n\\[p = \\frac{n_s}{n}\\]\n, where \\(n\\) - sample size and \\(n_s\\) - the number of successful observations (or it can be the number of failures).\n\\[SE = \\sqrt{\\frac{p \\cdot (1 - p)}{n}}\\] , where \\(SE\\) is standard error. To perform a test, one first needs to derive a Null hypothesis:\n\\[H_0: p = p_0\\]\nand an alternative hypothesis:\n\none-sided \\(H_A: p < p_0\\) or \\(H_A: p > p_0\\),\ntwo-sided \\(H_A: p \\ne p_0\\).\n\nFinally, we needs to calculate \\(Z\\) statistics:\n\\[Z = \\frac{p_0-p}{SE}\\]\nObtaining the value of \\(Z\\) enables us to either compute confidence intervals (\\(CI\\)) or reject \\(H_0\\) in favor of \\(H_A\\)"
  },
  {
    "objectID": "posts/binom-test-1/index.html#success-failure-condition",
    "href": "posts/binom-test-1/index.html#success-failure-condition",
    "title": "One-Sample Binomial Test (Part 1)",
    "section": "“Success-failure” condition",
    "text": "“Success-failure” condition\nTo approximate any distribution as normal, it is imperative to calculate the mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)). For the Binomial distribution, which consists of a number of experiments \\(n\\) and a probability \\(p\\), the mean of the normal distribution is:\n\\[\\mu = n \\cdot p\\]\nand the standard deviation:\n\\[\\sigma = \\sqrt{n \\cdot p \\cdot (1 - p)}.\\]\nMeeting the “Success-Failure” condition is crucial to approximate Binomial distribution as Normal. Below, I present an instance of 50 Binomial events with varying probability rates of 5%, 30%, and 90%. In tabs binomial vs normal binomial distributions are plotted against it’s normal distribution approximation.\n\nbinomial vs. normal (R)error (R)binomial vs. normal (Python)error (Python)\n\n\n\n\nCode\n# probabilities\np <- c(0.05, 0.3, 0.9)\n\n# successes\nx <- 0:50\n\n# create data.table\ndt <- CJ(p, x)\n\n# add size column\ndt[, size := 50]\n\n# add binomial probability\ndt[, Binomial := dbinom(x, size=size, prob=p) * 100]\n\n# create label column\ndt[, label := paste0(\"Prob: \", round(p*100),\n                     \"%, exp. successes \", round(p*size),\n                     \" exp. failures \", round((1-p)*size))]\n\n# calculate mean and standard deviation\ndt[, mu := size * p]\ndt[, st.dev := sqrt(p * (1 - p) * size)]\n\n# get norm distribution\ndt[, Normal := dnorm(x, mean=mu, sd = st.dev) * 100]\n\n# convert to ordered factor\ndt[, label := factor(label, levels=c(\"Prob: 5%, exp. successes 2 exp. failures 48\",\n                                     \"Prob: 30%, exp. successes 15 exp. failures 35\",\n                                     \"Prob: 90%, exp. successes 45 exp. failures 5\" ))]\n\n# reshape for plotting\ndt.plot <- melt(dt, id.vars = c(\"x\", \"label\"),\n                measure.vars = c(\"Binomial\", \"Normal\"),\n                variable.name = c(\"Type\"),\n                value.name = c('prob'))\n\n# create figure\nfig <- ggplot(dt.plot, aes(x, prob, color=Type)) + geom_point() + facet_wrap(~label, ncol = 1) + ylab(\"Probability, %\")\nfig\n\n\n\n\n\n\n\n\n\nCode\n# calculate error (use data.table from previous code chunk)\ndt[, Error := Binomial - Normal]\n\n# create figure\nfig <- ggplot(dt, aes(x, Error)) + geom_point() + facet_wrap(~label, ncol = 1) + ylab(\"Error (binom. - norm.), %\")\nfig\n\n\n\n\n\n\n\n\n\nCode\n# probabilities\np = [0.05, 0.3, 0.9]\n\n# successes\nx = np.arange(51)\n\n# create DataFrame with all combinations\ndf_1 = pd.DataFrame({'p': p})\ndf_2 = pd.DataFrame({'x': x})\n\n# create key for joining\ndf_1['key'] = 0\ndf_2['key'] = 0\n\n# perform cross join\ndf = df_1.merge(df_2, on='key', how='outer')\n\n# drop key column\ndel df['key']\n\n# add size value\ndf['size'] = 50\n\n# calculate binomial probability\ndf['Binomial'] = binom.pmf(df['x'], df['size'], df['p']) * 100\n \n# calculate mean and standard deviation\ndf['mu'] = df['size'] * df['p']\ndf['se'] = np.sqrt(df['p'] * (1 - df['p']) / df['size'])\ndf['std'] = np.sqrt(df['p'] * (1 - df['p']) * df['size'])\n\n# get norm distribution\ndf['Normal'] = df.apply(lambda x: norm.pdf(x['x'], x['mu'], x['std']) * 100, axis = 1)\n\n# create figure\nfig, ax = plt.subplots(3, 1, sharey=True)\n\n# iterate over probabilities\nfor i, _p in enumerate(p):\n  # select data for plotting\n  dt_plot = df.loc[df['p'] == _p].copy()\n  \n  # plot binomial and normal distributions\n  ax[i].plot(dt_plot['x'], dt_plot['Binomial'], \"o\", label='Binomial');\n  ax[i].plot(dt_plot['x'], dt_plot['Normal'], \"-\", label='Normal', linewidth=3);\n  \n  # add sub titles\n  ax[i].set_title(f\"Prob. {_p*100:.0f}% expected {50*_p:.0f} successes and {50*(1-_p):.0f} failures\", fontsize=8);\n\n# add labels\nax[1].set_ylabel(\"Probability, %\");\nax[2].set_xlabel(\"x\");\n\n# add legend and white background\nlegend = ax[1].legend(frameon = 1);\nframe = legend.get_frame();\nframe.set_color('white');\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nCode\n# calculate error (use DataFrame from previous code chunk)\ndf['Error'] = df.Binomial - df.Normal\n\n# create figure\nfig, ax = plt.subplots(3, 1, sharey=True)\n\n# iterate over probabilities\nfor i, _p in enumerate(p):\n  # select data for plotting\n  dt_plot = df.loc[df['p'] == _p]\n  \n  # plot binomial and normal distributions\n  ax[i].plot(dt_plot['x'], dt_plot['Error'], \"o\", color='k', markersize=3);\n  \n  # add sub titles\n  ax[i].set_title(f\"Prob. {_p*100:.0f}% expected {50*_p:.0f} successes and {50*(1-_p):.0f} failures\", fontsize=8);\n  \n# adjust limits for better readability\nax[0].set_ylim(-4.9, 4.9)\n\n# add labels\n\n\n(-4.9, 4.9)\n\n\nCode\nax[1].set_ylabel(\"Probability, %\")\nax[2].set_xlabel(\"x\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe first thing to notice is that the “Success-Failure” criterion is only met for the p=30% case. In tabs error one can see that approximation error, i.e. the difference between probabilities calculated using Binomial and Normal distribution approximations, is above 2% for the \\(p=5\\%\\) and \\(p=90\\%\\) examples."
  },
  {
    "objectID": "posts/binom-test-1/index.html#estimating-approximation-error",
    "href": "posts/binom-test-1/index.html#estimating-approximation-error",
    "title": "One-Sample Binomial Test (Part 1)",
    "section": "Estimating approximation error",
    "text": "Estimating approximation error\nLet’s dive deeper into understanding approximation error. We have to be mindful, when comparing discrete binomial distribution with a continuous normal distribution. The binomial distribution probability spaces bounded between 0 and k events, where k number of attempts while for normal it’s mathematical y unbounded (for large Z values you can calculate very small probability values). Apart for case \\(p=50\\%\\) binomial distribution is not symmetrical as oppose to normal distribution. In examples bellow, we will compare areas under the curve for both distributions using interactive charts by plotly library/package. First I have created a functions which visualizes both distributions using interactive charts.\n\nplot_ly function (R)plotly function (Python)\n\n\n\n\nCode\nPlotDistributions <- function(prob, size){\n  # create observation vector\n  if (prob < 0.5){\n    x <- 0:(size*prob*3)\n  } else {\n    x <- (size - 3 * size * (1 - prob)):size\n  }\n  \n  # get probabilities for both binomial and norm distributions\n  y.binom <- dbinom(x, size=size, prob=prob) * 100\n  y.norm <- dnorm(x, mean=size * prob, sd = sqrt(prob * (1 - prob) * size)) * 100\n  \n  # create data table for plotting\n  dt <- data.table(x = x,\n                   prob.binom = y.binom,\n                   prob.norm = y.norm)\n  \n  # add columns with hover information\n  dt[, text.1 := paste0('Point estimate probability<br>to observer exactly ', x,\n                        ' events is ', round(y.binom, 3), \"%\")]\n  dt[, text.2 := paste0('Point estimate probability<br>to observer exactly ', x,\n                        ' events is ', round(y.norm, 3), \"%\")]\n  dt[, text.3 := paste0('Point estimate error<br>to observer exactly ', x,\n                        ' events is ', round(y.binom - y.norm, 3), \"%\")]\n  # create figure\n  fig <- plot_ly(data = dt, type = 'scatter', mode = 'lines')\n  \n  # add traces\n  fig <- fig %>% add_trace(x = ~x, y = ~prob.norm, text = ~text.1,\n                           name = 'Normal',mode = 'lines',\n                           hoverinfo = 'text',\n                           line = list(color = \"#FF6666\", width = 5))\n  fig <- fig %>% add_trace(x = ~x, y = ~prob.binom, text = ~text.2,\n                           name = 'Binomial',mode = 'markers',\n                           hoverinfo = 'text',\n                           marker = list(color = \"#3399FF\", size = 12))\n  fig <- fig %>% add_trace(x = ~x, y = ~(prob.binom-prob.norm), text = ~text.3,\n                           name = 'Error',mode = 'lines+markers',\n                           hoverinfo = 'text',\n                           line = list(color = \"black\", width = 5),\n                           marker = list(color = \"black\", size = 12),\n                           visible = \"legendonly\") \n  \n  # update layout\n  fig <- fig %>% layout(title = paste0(\"p = \", round(prob*100), \"%, \", size, \" trials\"),\n                        xaxis = list(title = \"Observations\"),\n                        yaxis = list (title = \"Probability, %\"),\n                        hovermode = \"x unified\",\n                        legend=list(title=list(text='<b> Distributions </b>')))\n  # return figure\n  return(fig)\n}\n\n\n\n\n\n\nCode\n# create function for plotting distributions\ndef plot_distributions(prob, size):\n  # create observation vector\n  x = np.arange(int(size*prob*3))\n  \n  # get probabilities for both binomial and norm distributions\n  y_binom = binom.pmf(x, n=size, p=prob) * 100\n  \n  # calculate variance and sigma\n  variance = size * prob * (1 - prob)\n  sigma = np.sqrt(variance)\n  \n  y_norm = norm.pdf(x, loc = size * prob, scale = sigma) * 100\n  \n  # generate Data.Frame\n  df = pd.DataFrame({'x': x, 'binom': y_binom, 'norm': y_norm})\n  \n  # calculate error\n  df['error'] = df.binom - df.norm\n  error = df.binom - df.norm\n  \n  # generate hover messages\n  df['text_1'] = df.apply(lambda x: f'Point estimate probability<br>to observer exactly {x[\"x\"]:.0f} events is {x[\"binom\"]:.3f}%', axis = 1)\n  df['text_2'] = df.apply(lambda x: f'Point estimate probability<br>to observer exactly {x[\"x\"]:.0f} events is {x[\"norm\"]:.3f}%', axis = 1)\n  df['text_3'] = df.apply(lambda x: f'Point estimate error<br>to observer exactly {x[\"x\"]:.0f} events is {x[\"error\"]:.3f}%', axis = 1)\n  \n  # create figure\n  fig = go.Figure()\n  fig.add_trace(go.Scatter(x=x, y=y_norm, text = df['text_1'].values,\n                           mode='lines', name='Normal', hoverinfo = 'text',\n                           line=dict(color='#FF6666', width=5)))\n  fig.add_trace(go.Scatter(x=x, y=y_binom, text = df['text_2'].values,\n                           mode='markers', name='Binomial', hoverinfo = 'text',\n                           marker=dict(color='#3399FF', size=12)))\n  fig.add_trace(go.Scatter(x=x, y=error, text = df['text_3'].values,\n                           mode='markers', name='Error', hoverinfo = 'text',\n                           marker=dict(color='black', size=12),\n                           visible = \"legendonly\"))\n                           \n  # Edit the layout\n  fig.update_layout(title=f\"p = {prob*100:.1f}%, {size} trials\",\n                    xaxis_title='Observations',\n                    yaxis_title='Probability, %',\n                    template=\"ggplot2\",\n                    hovermode=\"x unified\")\n                   \n  return fig\n\n\n\n\n\nNext I provide examples using small probabilities (99%, 95%, 0.5% and 0.1%) where “Success-failure” condition is not met, i.e. either 5 failures or 5 successes are expected for all 4 examples.\n\n99.9% (R)99.5% (R)0.5% (Python)0.01% (Python)\n\n\n\n\nCode\n# return figure\nPlotDistributions(0.999, 5000)\n\n\n\n\n\n\n\n\n\n\nCode\n# return figure\nPlotDistributions(0.995, 1000)\n\n\n\n\n\n\n\n\n\n\nCode\n# use python helper function from above\nfig = plot_distributions(0.005, 1000)\nfig\n\n\n\n                        \n                                            \n\n\n\n\n\n\nCode\n# use python helper function from above\nfig = plot_distributions(0.001, 5000)\nfig\n\n\n\n                        \n                                            \n\n\n\n\n\nLet’s taken hypothetical model with small probability, i.e. we expect to have 1% manufacturing defects or 1% of clients in our portfolio will fail to meet their credit obligations. For this hypothetical model to meet “Success-Failure” condition we would need to collect at least 1000 observations, i.e. to manufacture 1000 devices or issue credit to 1000 obligatory. In the tab 1% (R) that if this condition is not met, i.e. we collected 500 events and expect 5 failures/defaults. From both visual inspection and reviewing error estimates for different outcomes (select error on legend to view error values) it’s clear that normal distribution approximation overestimates probability for \\(\\ge5\\) and underestimates \\(\\le5\\). For all 4 examples, expected number of “Successes-failures” is 5. One can observed that the smaller the probability is the larger the error becomes.\nLet’s compare 3 probabilities calculated using binomial and normal distribution approximation:\n\n\\(p_1\\) we would observe more than expected number of occurrences,\n\\(p_2\\) we would observe expected number of occurrences,\n\\(p_3\\) we would observe less than expected number of occurrences.\n\nWe can calculate these probabilities using both binomial and normal distributions and then compare them, e.g. \\(ratio_1 = \\frac{p_1 (binom.)}{p_1 (norm.)}\\). \\(p_1\\) probabilities and all 3 ratios are calculated for all 4 examples above and few more examples. For all cases we have 5 expected successes or failures. Results are presented in table bellow.\n\nTableRatios\n\n\n\n\nCode\n# generate data.table\ndt <- data.table(p = c(0.999, 0.995, 0.99, 0.95, 0.8,\n                       0.5,\n                       0.2, 0.05, 0.01, 0.005, 0.001),\n                 N = c(5000, 1000, 500, 100, 25,\n                       10,\n                       25, 100, 500, 1000, 5000))\n# expected events\ndt[, x := p * N]\n\n# get binomial, normal probabilities and their ratio\ndt[, binom.p1 := dbinom(x, size=N, prob=p) * 100]\ndt[, norm.p1 := dnorm(x, mean=N * p, sd = sqrt(p * (1 - p) * N)) * 100]\ndt[, ratio.1 := binom.p1/norm.p1 * 100]\n\n# calculate p2 probabilities\ndt[, binom.p2 := pbinom(x-1, size=N, prob=p) * 100]\ndt[, norm.p2 := (1-dnorm(x, mean=N * p, sd = sqrt(p * (1 - p) * N)))/2 * 100]\n\n# calculate p3 probabilities\ndt[, binom.p3 := (1 - pbinom(x, size=N, prob=p)) * 100]\n\n# calculate ratios\ndt[, ratio.2 := binom.p2/norm.p2 * 100]\ndt[, ratio.3 := binom.p3/norm.p2 * 100]\n\n# round columns for better readability\ncols <- names(dt)[4:11]\ndt[,(cols) := round(.SD, 3), .SDcols=cols]\n\n# convert probability to percents\ndt[, p := round(p * 100, 1)]\n\n# remove column\ndt$x <- NULL\n\n# generate table with first few rows and columns\nknitr::kable(dt[, c(1:5, 9, 10)], col.names = c(\"p, %\", \"N\", \"p_1 (binom.), %\", \"p_1 (norm.), %\", \"ratio_1\", \"ratio_2\", \"ratio_3\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n \n  \n    p, % \n    N \n    p_1 (binom.), % \n    p_1 (norm.), % \n    ratio_1 \n    ratio_2 \n    ratio_3 \n  \n \n\n  \n    99.9 \n    5000 \n    17.556 \n    17.850 \n    98.349 \n    93.497 \n    107.220 \n  \n  \n    99.5 \n    1000 \n    17.591 \n    17.886 \n    98.349 \n    93.538 \n    107.181 \n  \n  \n    99.0 \n    500 \n    17.635 \n    17.931 \n    98.349 \n    93.589 \n    107.132 \n  \n  \n    95.0 \n    100 \n    18.002 \n    18.305 \n    98.345 \n    94.008 \n    106.734 \n  \n  \n    80.0 \n    25 \n    19.602 \n    19.947 \n    98.267 \n    95.764 \n    105.099 \n  \n  \n    50.0 \n    10 \n    24.609 \n    25.231 \n    97.535 \n    100.832 \n    100.832 \n  \n  \n    20.0 \n    25 \n    19.602 \n    19.947 \n    98.267 \n    105.099 \n    95.764 \n  \n  \n    5.0 \n    100 \n    18.002 \n    18.305 \n    98.345 \n    106.734 \n    94.008 \n  \n  \n    1.0 \n    500 \n    17.635 \n    17.931 \n    98.349 \n    107.132 \n    93.589 \n  \n  \n    0.5 \n    1000 \n    17.591 \n    17.886 \n    98.349 \n    107.181 \n    93.538 \n  \n  \n    0.1 \n    5000 \n    17.556 \n    17.850 \n    98.349 \n    107.220 \n    93.497 \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nThere results show that when \\(p > 50\\%\\) overestimation is observed on the right side of distribution nd opposite is true for \\(p < 50\\%\\). The ratio between \\(p_1\\) probabilities is the same across all 3 examples. This is due to the fact that all 3 cases have 5 expected success/failures. Overestimation \\(ratio_2\\) are the same for \\(p > 50\\%\\) cases as underestimation \\(ratio_3\\). As implied probability becomes closer to 50% the binomial distribution becomes more symmetrical which can be supported by seeing that \\(ratio_2\\) and \\(ratio_3\\) are close to 100% for given example.\nFinally let’s compare cumulative probabilities for different examples of probabilities \\(p\\) and expected values (EV). I have added 95% (black dashed line) and 99% (red dashed line) confidence intervals to show when one can start rejecting null hypothesis using commonly used cut-off levels.\n\n0.1% (5 EV)10% (5 EV)10% (10 EV)20% (15 EV)50% (5 EV)50% (10 EV)50% (15 EV)80% (15 EV)90% (10 EV)99% (5 EV)99.9% (5 EV)\n\n\n\n\nCode\n# helper function for plotting\nvline <- function(x = 0, color = \"black\") {\n  list(\n    type = \"line\",\n    y0 = 0,\n    y1 = 1,\n    yref = \"paper\",\n    x0 = x,\n    x1 = x,\n    line = list(color = color, dash = \"dot\")\n  )\n}\n\n# helper function to draw cum. probabilities\nPlotCumProbs <- function(p, N){\n  \n  # for adjusting plot x limits\n  x.ratio <- p*N \n  \n  # create vector depending on probability\n  if (p <= 0.5){\n    x <- 0:max((N*p*3), N*p)\n  } else {\n    x <- max((N - 3 * N * (1 - p)), 0):N\n  }\n  \n  # calculate z values as events for different confidence intervals\n  Z.95.left <- (N * p - 1.960 * sqrt(p * (1 - p) * N))\n  Z.99.left <- (N * p - 2.576 * sqrt(p * (1 - p) * N))\n  Z.95.right <- (N * p + 1.960 * sqrt(p * (1 - p) * N))\n  Z.99.right <- (N * p + 2.576 * sqrt(p * (1 - p) * N))\n  \n  # calculate cum. probabilities for both binomial and normal distributions\n  y.binom <- pbinom(x, size=N, prob=p) * 100\n  y.norm <- pnorm(x, mean=N * p, sd = sqrt(p * (1 - p) * N)) * 100\n  \n  # create data.table for plotting\n  dt <- data.table(x = x, y.binom = y.binom, y.norm = y.norm)\n  # calculate error\n  dt[, error := y.binom - y.norm]\n  \n  # add hover text messages\n  dt[, text.1 := paste0(round(y.binom, 3), \"%\")]\n  dt[, text.2 := paste0(round(y.norm, 3), \"%\")]\n  dt[, text.3 := paste0(round(error, 3), \"%\")]\n  \n  # create figure\n  fig <- plot_ly(data = dt, type = 'scatter', mode = 'lines')\n  \n  # add traces\n  fig <- fig %>% add_trace(x = ~x, y = ~y.norm, text = ~text.1,\n                           name = 'Normal',mode = 'lines',\n                           hoverinfo = 'text',\n                           line = list(color = \"#FF6666\", width = 5))\n  fig <- fig %>% add_trace(x = ~x, y = ~y.binom, text = ~text.2,\n                           name = 'Binomial',mode = 'markers',\n                           hoverinfo = 'text',\n                           marker = list(color = \"#3399FF\", size = 12))\n  fig <- fig %>% add_trace(x = ~x, y = ~error, text = ~text.3,\n                           name = 'Error',mode = 'lines+markers',\n                           hoverinfo = 'text',\n                           line = list(color = \"black\", width = 5),\n                           marker = list(color = \"black\", size = 12))\n  \n  # update layout\n  fig <- fig %>% layout(title = paste0(\"p = \", round(p*100, 1), \"%, \", N, \" trials\"),\n                        xaxis = list(title = \"Observations\"),\n                        yaxis = list (title = \"Probability, %\"),\n                        hovermode = \"x unified\",\n                        shapes = list(vline(Z.99.left, color='red'), vline(Z.95.left), vline(Z.95.right), vline(Z.99.right, color='red')),\n                        legend=list(title=list(text='<b> Distributions </b>')))\n  \n  # return figure object\n  return(fig)\n}\n\n# plot example for 0.1%\nPlotCumProbs(0.001, 5000)\n\n\n\n\n\n\n\n\n\n\nCode\n# plot example for 1%\nPlotCumProbs(0.01, 500)\n\n\n\n\n\n\n\n\n\n\nCode\n# plot example for 10%\nPlotCumProbs(0.1, 100)\n\n\n\n\n\n\n\n\n\n\nCode\n# plot example for 20%\nPlotCumProbs(0.2, 75)\n\n\n\n\n\n\n\n\n\n\nCode\n# plot example for 50%\nPlotCumProbs(0.5, 10)\n\n\n\n\n\n\n\n\n\n\nCode\n# plot example for 50%\nPlotCumProbs(0.5, 20)\n\n\n\n\n\n\n\n\n\n\nCode\n# plot example for 50%\nPlotCumProbs(0.5, 30)\n\n\n\n\n\n\n\n\n\n\nCode\n# plot example for 80%\nPlotCumProbs(0.8, 75)\n\n\n\n\n\n\n\n\n\n\nCode\n# plot example for 90%\nPlotCumProbs(0.9, 100)\n\n\n\n\n\n\n\n\n\n\nCode\n# plot example for 99%\nPlotCumProbs(0.99, 500)\n\n\n\n\n\n\n\n\n\n\nCode\n# plot example for 99.9%\nPlotCumProbs(0.999, 5000)\n\n\n\n\n\n\n\n\n\nThe first thing to notice, that 2-sided hypothesis can’t be really tested for low EV example, see tabs with (5 EV) in their name - the red dashed lines are outside of viable outcome range with an exception of 50% (5EV). For this example, after 10 attempts rejecting Null hypothesis \\(H_0 \\ne p\\) with 99% CI is possible by observing 0 or 10 successes. AS sample size increases, i.e. larger EV, distribution widths are reduced, which means that 2-sided tests with 99% CI become viable, see example with (EV 15). Again, when comparing distributions with same EV but different probabilities, we see that error between binomial distribution and it’s approximation using normal distribution is very similar."
  },
  {
    "objectID": "posts/binom-test-1/index.html#min.-sample-size",
    "href": "posts/binom-test-1/index.html#min.-sample-size",
    "title": "One-Sample Binomial Test (Part 1)",
    "section": "Min. sample size",
    "text": "Min. sample size\nFinally let’s investigate for different probabilities how much \\(p_1\\) and \\(ratio_1\\) are affected as sample size is increased.\nNow let’s investigate how these probabilities and errors change as sample size is increased and “success-failure” condition is met.\n\nRatio 1Approximation error\n\n\n\n\nCode\n# generate data.table with expected values and sample probabilities\nprobs <- c(0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.45, 0.5, 0.55, 0.6, 0.7, 0.8, 0.9, 0.99, 0.999)\ndt <- CJ(Sucesses = c(1:50), p = probs)\n\n# get probability and sample size for expected values\ndt[p <= 0.5, N := ceiling(Sucesses / p)]\ndt[p > 0.5, N := ceiling(Sucesses / (1 - p))]\n\n# get number of failures\ndt[, Failures := N - Sucesses]\n\n# calculate probabilities to observe expected number of observations \ndt[p <= 0.5, binom.p1 := dbinom(Sucesses, size = N, prob=p) * 100]\ndt[p > 0.5, binom.p1 := dbinom(Failures, size = N, prob=p) * 100]\ndt[p <= 0.5, norm.p1 := dnorm(Sucesses, mean = N * p, sd = sqrt(p * (1 - p) * N)) * 100]\ndt[p > 0.5, norm.p1 := dnorm(Failures, mean = N * p, sd = sqrt(p * (1 - p) * N)) * 100]\n\n# get ratio between point estimates\ndt[, ratio.1 := round(binom.p1/norm.p1 * 100, 3)]\n\n# for coloring get probability\ndt[, Probability := paste0(round(p * 100, 1), \"%\")]\n\n# text message for hover\ndt[, text := paste0(Probability, \" with \", N, \" attempts: \", ratio.1)]\n\n# generate color palette\n# code example taken from\n# https://stackoverflow.com/questions/15282580/how-to-generate-a-number-of-most-distinctive-colors-in-r\nn <- length(probs)\nqual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]\ncol_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))\n\n# generate figure\nfig <- plot_ly(dt,\n               x = ~Sucesses,\n               y = ~ratio.1,\n               color = ~Probability,\n               colors = col_vector,\n               text = ~text,\n               hoverinfo = \"text\",\n               line = list(width = 4)) \nfig <- fig %>% add_lines()\n\n# with log scales\nfig <- layout(fig,\n              hovermode = \"x unified\",\n              yaxis = list(type = \"log\", title = \"Ratio 1, %\"),\n              xaxis = list(title = \"Expected sucessess/failures\"),\n              legend=list(title=list(text='<b> Probabilities </b>')))\n\nfig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe accuracy of approximating binomial distribution with binomial mostly depends on sample size, i.e. expected number of successes/failures."
  },
  {
    "objectID": "posts/binom-test-1/index.html#conclusions",
    "href": "posts/binom-test-1/index.html#conclusions",
    "title": "One-Sample Binomial Test (Part 1)",
    "section": "Conclusions",
    "text": "Conclusions\nOne should be very mindful when designing experiments for one-sample binomial test. Failing to meet “success-failure” condition would result in overestimation or underestimation of you \\(H_0\\) hypothesis results."
  },
  {
    "objectID": "posts/binom-test-1/index.html#references",
    "href": "posts/binom-test-1/index.html#references",
    "title": "One-Sample Binomial Test (Part 1)",
    "section": "References",
    "text": "References\nContent for this blog post was prepared using following references:\n\nhttps://statistics.laerd.com/spss-tutorials/binomial-test-using-spss-statistics.php\nhttp://mlwiki.org/index.php/Binomial_Proportion_Tests\nhttps://sites.ualberta.ca/~lkgray/uploads/7/3/6/2/7362679/slides_-_binomialproportionaltests.pdf\nhttps://www.technologynetworks.com/informatics/articles/the-binomial-test-366022\nhttps://towardsdatascience.com/turning-a-two-sample-event-rate-test-into-a-one-sample-binomial-test-23fbfb9d1df6\nhttps://www.studysmarter.us/explanations/math/statistics/binomial-hypothesis-test/\nhttps://www.statology.org/success-failure-condition/\nhttps://towardsdatascience.com/bernoulli-and-binomial-random-variables-d0698288dd36\nhttps://ubc-mds.github.io/DSCI_551_stat-prob-dsci/lectures/simulation.html\nhttps://math.stackexchange.com/questions/1978138/probability-of-x-red-balls-when-drawing-y-balls-from-a-red-and-b-green-balls"
  },
  {
    "objectID": "posts/google-analytics-capstone/index.html",
    "href": "posts/google-analytics-capstone/index.html",
    "title": "Exports in Lithuania",
    "section": "",
    "text": "Hello and welcome to my first blog post on Quarto! As a part of my Google Data Analytics specialization course capstone project, I have created this blog. In this post, I will be exploring the changes in historical exports and imports of Lithuania over time, using the 6-step framework presented in the course. I hope you enjoy reading about my findings and analysis. Let’s begin!"
  },
  {
    "objectID": "posts/google-analytics-capstone/index.html#ask",
    "href": "posts/google-analytics-capstone/index.html#ask",
    "title": "Exports in Lithuania",
    "section": "Ask",
    "text": "Ask\nBefore diving into data processing and visualization, it’s essential to take note of the following key events: the Covid-19 pandemic since spring 2020, the strained relationship between China and Lithuania due to the Taiwan question, and the Russian invasion of Ukraine in 2022.\nThe Covid-19 pandemic has brought unprecedented changes to the global economy, and Lithuania was no exception. The outbreak caused major disruptions in global trade, resulting in a decline in demand for Lithuanian goods and services. On the other hand, Lithuania’s relationship with China has been deteriorating due to the Taiwan question. This has led to a decrease in exports to China, one of Lithuania’s top trading partners.\nThe Russian invasion of Ukraine in 2022 has also had a significant impact on Lithuania’s trade patterns. The conflict has resulted in the imposition of economic sanctions on Russia, affecting Lithuania’s trade with its Eastern neighbor. The situation is still developing, and it will be interesting to see how Lithuania’s trade with Russia evolves in the coming years."
  },
  {
    "objectID": "posts/google-analytics-capstone/index.html#prepare",
    "href": "posts/google-analytics-capstone/index.html#prepare",
    "title": "Exports in Lithuania",
    "section": "Prepare",
    "text": "Prepare\nThe data was downloaded from the Lithuanian National website in an Excel file format. You can also find the same file in my blog’s Github repo. It was then loaded into the R environment using the tidyverse library and converted to data.table format.\n\n\nCode\n# load libraries\nlibrary(readxl)\nlibrary(data.table)\nlibrary(kableExtra)\nlibrary(plotly)\nlibrary(zoo)\n\n# read data from excel\ndt <- read_excel(\"lb_data.xlsx\", sheet = 1, range = \"A13:AN201\")\n\n# convert to data.table object\ndt <- data.table(dt)\n\n# rename columns for convinience\nsetnames(dt, c(\"...1\"), \"Type\")\n\n# shift columns to get correct names\ndt[, Country := rep(dt[seq(1, nrow(dt), 4), Type], each = 4)]\ndt <- dt[!(seq(1, nrow(dt), 4))]\n\n# generate table with first few rows and columns\nkbl(dt[1:6, c(41, 1, 38, 39, 40)]) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n \n  \n    Country \n    Type \n    Q1/2022 \n    Q2/2022 \n    Q3/2022 \n  \n \n\n  \n    Total \n    Exports \n    3666.97 \n    4230.51 \n    4615.79 \n  \n  \n    Total \n    Imports \n    2417.61 \n    2585.59 \n    2814.54 \n  \n  \n    Total \n    Balance \n    1249.36 \n    1644.92 \n    1801.25 \n  \n  \n    European Union (27 countries) \n    Exports \n    2712.50 \n    3182.83 \n    3343.46 \n  \n  \n    European Union (27 countries) \n    Imports \n    1646.39 \n    1887.25 \n    2026.76 \n  \n  \n    European Union (27 countries) \n    Balance \n    1066.11 \n    1295.58 \n    1316.70 \n  \n\n\n\n\n\nIn the table above, you can see the total exports and imports of all countries, as well as those of the European Union’s (27 countries) for the three most recent quarters. Even before data cleaning and preparation, a visual analysis shows that a majority of exports and imports go to EU countries. Data is available about 41 unique countries."
  },
  {
    "objectID": "posts/google-analytics-capstone/index.html#process",
    "href": "posts/google-analytics-capstone/index.html#process",
    "title": "Exports in Lithuania",
    "section": "Process",
    "text": "Process\nFor this project, we didn’t need to do much data cleaning or processing since there were no missing values or unknown formats. However, we did make some minor formatting adjustments in the Prepare section. To keep things simple, I decided not to split this section into different parts."
  },
  {
    "objectID": "posts/google-analytics-capstone/index.html#analyze",
    "href": "posts/google-analytics-capstone/index.html#analyze",
    "title": "Exports in Lithuania",
    "section": "Analyze",
    "text": "Analyze\nAfter formatting and sorting the data, I investigated export changes of top 10 largest export destinations in 2003 Q3. Exports across European countries saw a substantial increase, with some countries experiencing growth rates of between 200-700%, exports to Belarus and Russia dropped significantly by 60% and 50%, respectively.\n\n\nCode\n# select exports data and exclude aggregated entires\nagg <- c(\"Total\", \"European Union (27 countries)\", \"Euro Area (18 countries)\", \"Commonwealth of Independent States\", \"Offshore financial centers\", \"Other countries\")\ndt.export <- copy(dt[Type == \"Exports\" & !Country %in% agg])\ndt.export[, `Change, %` := round((`Q3/2022` / `Q3/2013` - 1) * 100, 1)]\n# get top 10 starting and last period countries by export\nstart <- tail(dt.export[, c(41, 4, 40)][order(`Q3/2013`)], 10)\n# generate kable table\nkbl(start, caption = \"Top 10 largest export destinations in 2013 Q3\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\nTop 10 largest export destinations in 2013 Q3\n \n  \n    Country \n    Q3/2013 \n    Q3/2022 \n  \n \n\n  \n    United Kingdom \n    44.43 \n    261.90 \n  \n  \n    Netherlands \n    44.60 \n    313.90 \n  \n  \n    Norway \n    46.59 \n    115.63 \n  \n  \n    France \n    47.51 \n    380.13 \n  \n  \n    Denmark \n    61.80 \n    214.14 \n  \n  \n    Latvia \n    78.22 \n    181.07 \n  \n  \n    Poland \n    80.27 \n    209.38 \n  \n  \n    Belarus \n    120.38 \n    45.89 \n  \n  \n    Germany \n    148.49 \n    637.61 \n  \n  \n    Russia \n    323.90 \n    154.09 \n  \n\n\n\n\n\nI found it interesting to observe that out of the 41 countries with known data, there were export reductions in only four countries: Egypt, Russia, Belarus, and Japan. Furthermore, I noticed that the largest export changes were seen in countries with relatively small exports in 2003 Q3, such as Croatia and Malta, whose exports increased 40 times. It’s worth noting that Canada, a large economy, also saw a significant increase in exports, which grew almost 30 times.\n\n\nCode\n# get top 10 starting and last period countries by export\ntop_change <- tail(dt.export[, c(41, 4, 40, 42)][order(`Change, %`)], 5)\nlow_change <- head(dt.export[, c(41, 4, 40, 42)][order(`Change, %`)], 5)\n\n# get changes\ndt.changes <- rbind(low_change, top_change)\n\n# generate kable table\nkbl(dt.changes, caption = \"Top 5 largest positive and negative export changes\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\nTop 5 largest positive and negative export changes\n \n  \n    Country \n    Q3/2013 \n    Q3/2022 \n    Change, % \n  \n \n\n  \n    Egypt \n    4.65 \n    0.94 \n    -79.8 \n  \n  \n    Belarus \n    120.38 \n    45.89 \n    -61.9 \n  \n  \n    Russia \n    323.90 \n    154.09 \n    -52.4 \n  \n  \n    Japan \n    1.64 \n    1.21 \n    -26.2 \n  \n  \n    India \n    2.52 \n    3.03 \n    20.2 \n  \n  \n    Bulgaria \n    0.76 \n    13.92 \n    1731.6 \n  \n  \n    Canada \n    0.38 \n    11.62 \n    2957.9 \n  \n  \n    Romania \n    0.70 \n    27.03 \n    3761.4 \n  \n  \n    Malta \n    0.50 \n    21.17 \n    4134.0 \n  \n  \n    Croatia \n    0.11 \n    5.39 \n    4800.0 \n  \n\n\n\n\n\nRegarding imports, I observed that imports were reduced to only three countries: Finland, Belarus, and Russia. It is worth noting that imports from Japan increased by 80%, which is in contrast to the reduced exports by 26%. Lastly, I observed that imports from Portugal increased by 35 times.\n\n\nCode\n# select exports data and exclude aggregated entires\nagg <- c(\"Total\", \"European Union (27 countries)\", \"Euro Area (18 countries)\", \"Commonwealth of Independent States\", \"Offshore financial centers\", \"Other countries\")\ndt.import <- copy(dt[Type == \"Imports\" & !Country %in% agg])\ndt.import[, `Change, %` := round((`Q3/2022` / `Q3/2013` - 1) * 100, 1)]\n\n# get top 10 starting and last period countries by export\ntop_change <- tail(dt.import[, c(41, 4, 40, 42)][order(`Change, %`)], 5)\nlow_change <- head(dt.import[, c(41, 4, 40, 42)][order(`Change, %`)], 5)\n\n# get changes\ndt.changes <- rbind(low_change, top_change)\n\n# generate kable table\nkbl(dt.changes, caption = \"Top 5 largest positive and negative import changes\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\nTop 5 largest positive and negative import changes\n \n  \n    Country \n    Q3/2013 \n    Q3/2022 \n    Change, % \n  \n \n\n  \n    Belarus \n    132.20 \n    35.61 \n    -73.1 \n  \n  \n    Russia \n    121.83 \n    41.64 \n    -65.8 \n  \n  \n    Finland \n    43.22 \n    28.25 \n    -34.6 \n  \n  \n    Denmark \n    38.90 \n    63.68 \n    63.7 \n  \n  \n    Japan \n    0.72 \n    1.31 \n    81.9 \n  \n  \n    Germany \n    43.94 \n    252.26 \n    474.1 \n  \n  \n    Luxembourg \n    1.58 \n    21.70 \n    1273.4 \n  \n  \n    Hong Kong \n    0.67 \n    9.97 \n    1388.1 \n  \n  \n    Malta \n    4.60 \n    91.82 \n    1896.1 \n  \n  \n    Portugal \n    0.59 \n    21.27 \n    3505.1 \n  \n\n\n\n\n\nOverall, this analysis highlights the importance of careful observation and analysis of data to uncover trends and patterns that can inform strategic decision-making for businesses and policymakers alike."
  },
  {
    "objectID": "posts/google-analytics-capstone/index.html#share",
    "href": "posts/google-analytics-capstone/index.html#share",
    "title": "Exports in Lithuania",
    "section": "Share",
    "text": "Share\nLet’s start from visualizing total export and import changes over time.\n\n\nCode\n# select data for plotting\ndt.plot.1 <- melt(dt, id.vars = c(\"Country\", \"Type\"), variable.name = \"Quarter\")\n\n# convert to datetime\ndt.plot.1[, Quarter := as.Date(as.yearqtr(Quarter, format = \"Q%q/%Y\"), frac = 1)]\n\n# colors for plotting\npal <- c(\"#6699ff\", \"#ff6666\")\n\n# create figure\nfig <- plot_ly(data = dt.plot.1[Type != \"Balance\" & Country == 'Total'],\n               type = 'scatter', mode = 'lines+markers',\n               x = ~Quarter, y = ~value, color = ~Type, colors = pal)\n# update layout\nfig <- fig %>% layout(title = \"Total import and export volumes over time\",\n                      xaxis = list(title = \"Date\"),\n                      yaxis = list (title = \"Volumne, M EUR\"))\nfig\n\n\n\n\n\n\nThere has been a significant increase in both total imports and exports, with a four-fold increase in imports and a three-fold increase in exports. However, during the first three quarters of 2020, there was a marked decline in both exports and imports, which can be attributed to the impact of Covid-19.\n\n\nCode\n# select data for plotting\ndt.plot.2 <- copy(dt.plot.1[Country %in% c(\"Total\", \"European Union (27 countries)\", \"Russia\", \"China\")])\n\n# reshape table\ndt.plot.2 <- dcast(dt.plot.2, Quarter+Type~Country, value.var = \"value\")\n\n# calculate share to Total values\ndt.plot.2[, `EU (27 countries) share, %` := round(`European Union (27 countries)` / Total * 100, 2)]\ndt.plot.2[, `Russia share, %` := round(`Russia` / Total * 100, 2)]\ndt.plot.2[, `China share, %` := round(`China` / Total * 100, 2)]\n\n# colors\neu.color <- 'rgb(22, 96, 167)'\nru.color <- 'rgb(205, 12, 24)'\nch.color <- 'rgb(0, 128, 0)'\n\n# create figure (imports)\nfig <- plot_ly(dt.plot.2[Type == 'Imports'],\n               x = ~Quarter, y = ~`EU (27 countries) share, %`,\n               name = 'EU (27 countries) imports', type = 'scatter', mode = 'lines',\n               line = list(color = eu.color, width = 4))\nfig <- fig %>% add_trace(y = ~`Russia share, %`,\n                         name = 'Russia imports',\n                         line = list(color = ru.color, width = 4))\nfig <- fig %>% add_trace(y = ~`China share, %`,\n                         name = 'China imports',\n                         line = list(color = ch.color, width = 4))\n# add exports\nfig <- fig %>% add_trace(x = dt.plot.2[Type == 'Exports']$Quarter,\n                         y = dt.plot.2[Type == 'Exports']$`EU (27 countries) share, %`,\n                         name = 'EU (27 countries) exports',\n                         line = list(color = eu.color, width = 4, dash = 'dash'))\nfig <- fig %>% add_trace(x = dt.plot.2[Type == 'Exports']$Quarter,\n                         y = dt.plot.2[Type == 'Exports']$`Russia share, %`,\n                         name = 'Russia exports',\n                         line = list(color = ru.color, width = 4, dash = 'dash'))\nfig <- fig %>% add_trace(x = dt.plot.2[Type == 'Exports']$Quarter,\n                         y = dt.plot.2[Type == 'Exports']$`China share, %`,\n                         name = 'China exports',\n                         line = list(color = ch.color, width = 4, dash = 'dash'))\n\n# update layout\nfig <- fig %>% layout(title = \"Total import and export volumes over time\",\n                      xaxis = list(title = \"Date\"),\n                      yaxis = list (title = \"Share to total, %\"))\nfig\n\n\n\n\n\n\nThe European Union has seen a significant increase in the proportion of imports in their total volume, which has increased by 50%. This means that the EU is becoming more reliant on imports and is potentially exporting less to other countries.\nWhen it comes to trade with Russia, both imports and exports have been declining steadily even before the start of the 2022 war. Prior to the war, Lithuania was a net importer with Russian trade. Surprisingly, tensions between China and Lithuania did not have a significant impact on their trade relationship, and the share of total imports from China increased almost 5 times.\nFinally let’s analyze how imports and exports were affected with Countries close to Russia.\n\n\nCode\n# countries to select\ncountry.list <- c(\"Kazakhstan\", \"Belarus\", \"Russia\", \"Turkey\", \"Offshore financial centers\")\n\n# select data for plotting\ndt.plot.3 <- copy(dt.plot.1[Country %in% country.list])\n\n# reshape table\ndt.plot.3 <- dcast(dt.plot.3, Quarter+Type~Country, value.var = \"value\")\n\n# reshape back\ndt.plot.3 <- melt(dt.plot.3, id.vars = c(\"Quarter\", \"Type\"), variable.name = \"Country\")\n\n# create new label\ndt.plot.3[, label := paste(Country, Type)]\n\n# color dictionary\ncolor.dict <- c(\"#C5AFA4\", \"#CC7E85\", \"#CF4D6F\", \"#A36D90\", \"#76818E\")\n\n# left figure\nfig1 <- plot_ly(data = dt.plot.3[Type == \"Imports\"],\n                x = ~Quarter, y = ~value, color=~label, colors = color.dict,\n                type = 'scatter', mode = 'lines+markers',\n                marker = list(line = list(width = 3)))\n\n# right figure\nfig2 <- plot_ly(data = dt.plot.3[Type == \"Exports\"],\n                x = ~Quarter, y = ~value, color=~label,\n                colors = color.dict,\n                type = 'scatter', mode = 'lines+markers',\n                line = list(dash = 'dot'),\n                marker = list(line = list(width = 3)))\n\nfig <- subplot(fig1, fig2, nrows = 2) %>% \n  layout(title = 'Exports and imports with countries close to Russia')\nfig\n\n\n\n\n\n\nNow, let’s turn our attention to how trade with countries close to Russia has been affected. Since the Russian invasion of Ukraine, imports and exports with Turkey and Kazakhstan have increased 3-5 times. This increase may be attributed to companies trying to circumvent sanctions to Russia through exports to Kazakhstan. It’s interesting to see how companies are adapting to changes in trade relationships and finding new opportunities to continue doing business."
  },
  {
    "objectID": "posts/google-analytics-capstone/index.html#act",
    "href": "posts/google-analytics-capstone/index.html#act",
    "title": "Exports in Lithuania",
    "section": "Act",
    "text": "Act\nIn conclusion, my analysis offers insights that could be utilized in evaluating the impact of historical events on Lithuania’s imports and exports. Based on the findings, several conclusions can be drawn:\n\nFirstly, the declines in imports and exports with Russia preceded the 2022 invasion to Ukraine. This finding suggests that the declining trade relationship between the two countries started since 2014 Crimea annexation.\nSecondly, the analysis revealed that political tensions over Taiwan did not have a substantial impact on exports to China.\nFinally, the data indicates that imports and exports to Kazakhstan have increased, potentially due to sanctions against Russia. This finding underscores the importance of considering the geopolitical context when imposing economical sanctions.\n\nOverall, the present study highlights the complexities of trade relationships between Lithuania and other countries."
  }
]